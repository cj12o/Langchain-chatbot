# ğŸš€ Langchain Chatbot

A **personal chatbot project** built to learn and apply practical concepts in conversational AI, LangChain, vector stores, and modern backend/frontend development.

---

## ğŸ“ **Project Structure**
<pre>
Langchain_chatbot/
â”œâ”€â”€ backend/ # FastAPI or server-side code
â”‚ â”œâ”€â”€ main.py # Example backend entry point
â”‚ â””â”€â”€ ...
â”œâ”€â”€ frontend/ # Frontend UI (Streamlit / Gradio / React)
â”‚ â”œâ”€â”€ Home.py # Example UI entry point
â”‚ â””â”€â”€ ...
â”œâ”€â”€ chroma_db_dir/ # Local vector DB storage (ignored in Git)
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ .gitignore # Ignored files and folders
â””â”€â”€ README.md # This file
</pre>


---

## ğŸ¯ **Project Objective**

The goal of this project is to:
- ğŸ§  Learn how to build production-style chatbot apps using **LangChain**
- âš™ï¸ Integrate **sessional context** and **RAG (Retrieval-Augmented Generation)**
- ğŸ—‚ï¸ Use a **Chroma vector database** to store embeddings for context retrieval
- ğŸ’» Create a **frontend UI** for smooth user interaction
- ğŸ”„ Maintain conversation context across sessions

---

ğŸ”‘ Key Features:
- Session-based context memory â€“ maintains conversational flow within and across sessions
- Previous session RAG toggle â€“ reuse knowledge from past chats efficiently
- Automated summarization â€“ at session end, summarises chats to build an optimized vector DB for future sessions
- Real-time token streaming â€“ significantly reduces Time To First Token (TTFT) and improves user experience
- Detailed metrics logging â€“ TTFT, input/output tokens per response to analyze performance
- Clean UI â€“ view chat history session-wise, date-wise, or in entirety, and export in JSON format
- Reset session context on-demand â€“ no need to restart sessions manually

ğŸ› ï¸ Tech Stack:
- LangChain â€“ prompt templates and plug-and-play model integration(run any llm)
- FastAPI â€“ backend APIs
- Streamlit â€“ intuitive frontend UI
- MongoDB â€“ chat and metrics storage
- LLMs â€“ locally run via Ollama & LM Studio:
- nomic-embed-text:v1.5 for embeddings
- hermes-3-llama-3.2-3b as the main chat completion model


## âš™ï¸ **Setup Guide**

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/YOUR_USERNAME/Langchain_chatbot.git
cd Langchain_chatbot
```
2ï¸âƒ£ Create a virtual environment
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/macOS
python3 -m venv venv
source venv/bin/activate
```
3ï¸âƒ£ Install Python dependencies
```bash
pip install -r requirements.txt
```


4ï¸âƒ£ Run the backend
```bash
cd backend
uvicorn main:app --reload
```

5ï¸âƒ£ Run the frontend
```bash
streamlit run Home.py
```

6ï¸âƒ£#Additional setup

-Local run model hermes serves as chat completion model (run via Lm studio)
-ocal run model nomic serves as embedding model (run via ollama cli)

ğŸ‘¤ Author
Chitransh Jain (cj12o)
ğŸ”— LinkedIn <https://www.linkedin.com/in/chitransh-jain-71bbb3336/>
ğŸ“§jchitransh@gmail.com


â­ Support
If you like this project, feel free to â­ star the repo and connect on LinkedIn!
